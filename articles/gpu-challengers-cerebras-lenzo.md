---
title: "LFM 2.5からAI半導体の世界を覗いてみた"
emoji: "🔥"
type: "tech"
topics: ["ai", "半導体", "cerebras", "lenzo", "cuda"]
published: false
---

## はじめに

> 業務自動化Pythonエンジニア。バイブコーディング歴1年 ≒ エンジニア歴。

### きっかけ：LFM 2.5を試した

先日、Liquid AIの超小型LLM「LFM 2.5-JP」を試した。

前の記事では「続きをどうぞ」で壊れてダメダメだったけど、他の7BモデルのLLMと比べたら確かに日本語がめちゃめちゃ綺麗だった。

色々調整したらまた面白そう。（これはまた今度）

### Lenzoの話を聞いて「何もわかってない」と気づいた

そこから色々流れてくる情報の中で、**Lenzo**という日本企業の話を初めて聞いた。

この話を聞いて、AI半導体について全く理解していないことに気づいた。

私の理解：
- **GPU**：言わずもがな、並列処理に強い
- **NPU**：最近聞かないけど、ローカルLLMに強い？
- **TPU**：Googleが作ったすごいやつ
- **WSE**：Cerebrasのやつ。去年から推してる
- **Lenzo**：...？全くわからない

### AIと一緒に調べてみた

というわけで、Claude先生と一緒にAI半導体の世界を調べてみた。

## AI半導体マップ

調べてみたら、こういう世界観だった。

```
【汎用GPU】
  NVIDIA H100/B200 - 学習も推論も何でも、CUDAエコシステム

【クラウド専用ASIC】
  Google TPU     - 学習特化、Google専用
  AWS Trainium   - 学習特化、AWS専用（Claudeもここで動いてる）
  AWS Inferentia - 推論特化、AWS専用

【スタートアップ・特化型】
  Cerebras WSE - 超大規模学習、ウェーハ丸ごと
  Groq LPU     - 推論特化、爆速（NVIDIAが200億ドルで買収）
  Lenzo CGLA   - 推論特化、省電力

【エッジ・組み込み】
  NPU - スマホやPCに内蔵。Apple Neural Engine、Qualcomm NPUなど
```

NPU、最近聞かないどころか今やどこにでも入ってた。

## 気づき1：チップの種類

チップにも種類があることがわかった。

```
柔軟性 高い ←――――――――――――――→ 低い
           GPU    CGRA    CGLA    ASIC

効率   低い ←――――――――――――――→ 高い
           GPU    CGRA    CGLA    ASIC
```

- **GPU**：何でもできる（汎用）
- **CGRA**：粗粒度で再構成可能（2次元グリッド）
- **CGLA**：線形配列でシンプル（Lenzoが採用）
- **ASIC**：1つの用途に完全特化（Google TPUなど）

## 気づき2：学習特化と推論特化

AI半導体には**学習特化**と**推論特化**がある。

| 種類 | 特徴 | 例 |
|------|------|-----|
| 学習特化 | 大量データで重みを更新 | Cerebras WSE、AWS Trainium |
| 推論特化 | 学習済みモデルで予測 | Lenzo CGLA、AWS Inferentia、Groq LPU |

なるほど、Cerebrasは学習、Lenzoは推論。そもそも土俵が違った。

やっぱりNVIDIAじゃなく別のアプローチの方がいいのかな？と思ったら...

## CUDAという壁

NVIDIAが強いのは、GPUの性能だけじゃなかった。

**CUDA**というソフトウェアエコシステムが本当の強み。

- TensorFlow、PyTorchなど主要フレームワークがCUDA前提
- 開発者の学習コスト・移行コストが高い
- 「CUDAで動く」というだけで選ばれる

いくらハードウェアが優れていても、CUDAエコシステムに乗れないと採用されにくい。

やっぱりNVIDIA強い。

## Lenzoについて

今回の調査のきっかけになった日本発のAI半導体スタートアップ。

AI計算では、演算そのものよりも**データ移動**に多くの電力が消費される。GPUはメモリアクセスが頻繁に発生する構造で、推論用途ではオーバースペックになりやすい。

Lenzoは「データを動かさない」ことで電力効率を向上。演算ユニットの近くにデータを配置して、長距離メモリアクセスを回避する。

推論特化・省電力。エッジデバイスや電力制約の厳しい環境向け。

参考: [日本発AI半導体スタートアップ "Lenzo" - note](https://note.com/neo_tech_world/n/n547457d07c11)

## 他のアプローチおさらい

### Google TPU

Googleが自社で作った学習特化チップ。Google専用。

### AWS Trainium / Inferentia

AWSが作ったチップ。Trainiumは学習特化、Inferentiaは推論特化。Claudeもここで動いてる。

### Cerebras WSE

ウェーハ全体を1つの巨大チップにする。NVIDIA H100の56倍のサイズ。超大規模モデルの学習に特化。

### Groq LPU

「決定論的アーキテクチャ」でGPUの2〜10倍のトークン生成速度。推論特化・爆速。

...で、このGroq LPUが面白かった。

## LPUの話

Groq LPUについて全く知らなかった。

調べてみたら、とんでもない技術だった。

- GPUは「次に何をするか」を実行時に判断
- LPUは「何をいつやるか」を事前に全部決定

だから遅延が完全に予測可能で、爆速になる。

ちなみに、X社の「Grok」とは全くの別物（名前が似てるだけ）。

## NVIDIAがLPUを買収した

2025年12月、NVIDIAがGroqを**200億ドル**で買収した。

「推論で勝てない」と判断して、技術ごと買ったということ。

## Rubinへの道

そして調べていくうちに、最近よく聞く「**Rubin**」がここに繋がることがわかった。

RubinはNVIDIAの次世代アーキテクチャ。Groqの技術を統合して、学習も推論も両方強化するらしい。

GPU一強時代はまだ続くのかもしれない。

## まとめ

LFM 2.5から始まって、AI半導体の世界を覗いてみた。

**学んだこと：**
- NVIDIAの強さはGPU性能だけじゃない。CUDAエコシステムが本当の壁
- チップにはGPU→CGRA→CGLA→ASICという柔軟性/効率のスペクトラムがある
- 学習特化と推論特化で土俵が違う
- Groq LPUはNVIDIAが200億ドルで買収するほどの技術だった
- そしてRubinへ繋がる

挑戦者たちの「別の山を登る」戦略は面白い。でもNVIDIAは技術を買収して取り込んでいく。

AI半導体の世界、もっと勉強していきたい。

## 参考リンク

- [NVIDIA を超える AI チップ Cerebras の全貌 - Zenn](https://zenn.dev/because02and/articles/cerebras-overview)
- [日本発AI半導体スタートアップ "Lenzo" - note](https://note.com/neo_tech_world/n/n547457d07c11)
- [Cerebras 公式](https://www.cerebras.ai/)
